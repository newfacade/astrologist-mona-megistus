{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic linear regression with one variable:\n",
    "\n",
    "$$y=\\theta_{0} + \\theta_{1}x$$\n",
    "\n",
    "what if linear model could not nicely fit training examples?<br>\n",
    "we can naturally extend linear model to polynomial model, for example:\n",
    "\n",
    "$$y=\\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} + \\theta_{3}x^{3}$$\n",
    "\n",
    "this method can be conclude as: map original attibutes x to some new set of quantities $\\phi(x)$ (called features), and use the same set of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 least mean squares with features\n",
    "let $\\phi \\in : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}$ be a feature map, then original batch gradient descent:\n",
    "\n",
    "$$\\theta := \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})x^{(i)}$$\n",
    "\n",
    "using features:\n",
    "\n",
    "$$\\theta := \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)})$$\n",
    "\n",
    "the above becomes computationally expensive when $\\phi(x)$ is high dimensional.<br>\n",
    "but we can observe that, if at some point , $\\theta$ can be represented as:\n",
    "\n",
    "$$\\theta = \\sum_{i=1}^{n}\\beta_{i}\\phi(x^{(i)})$$\n",
    "\n",
    "then in the next round:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta &:= \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)}) \\\\\n",
    "&=\\sum_{i=1}^{n}\\beta_{i}\\phi(x^{(i)}) + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)}) \\\\\n",
    "&=\\sum_{i=1}^{n}(\\beta_{i} + \\alpha(y^{(i)} - \\theta^{T}\\phi(x^{(i)})))\\phi(x^{(i)})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\theta$ can be also represented as a linear representation of $\\phi(x^{(i)})$<br>\n",
    "we can then derive $\\beta$'s update rule:\n",
    "\n",
    "$$\\beta_{i} := \\beta_{i} + \\alpha(y^{(i)} - \\sum_{j=1}^{n}\\beta_{j}\\phi(x^{(j)})^{T}\\phi(x^{(i)}))$$\n",
    "\n",
    "we only need to compute $\\left \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\right \\rangle = \\phi(x^{(j)})^{T}\\phi(x^{(i)}))$ to update parameters no matter how big the feature dimension p is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 kernel\n",
    "\n",
    "we define the kernel corresponding to the feature map $\\phi$ as a function that satisfying:\n",
    "\n",
    "$$K(x, z) = \\left \\langle \\phi(x), \\phi(z) \\right \\rangle$$\n",
    "\n",
    "for least mean square problem, we have:\n",
    "\n",
    "kernel's condition(mercer): let $ K: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\mapsto \\mathbb{R}$. then for K be a valid kernel, it is necessary and sufficient that for any $\\left \\{ x^{(1)},...,x^{(n)} \\right \\} $, the corresponding kernel matrix is symmetric positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 margins\n",
    "\n",
    "consider logistic regression, where the probability $p(y=1|x;\\theta)$ is modeled by $h_{\\theta}(x)=\\sigma(\\theta^{T}x)$. we predict 1 on an input x if and only if $\\theta^{T}x >= 0.5$, the larger $\\theta^{T}x$ is, the more confidence we are.<br>\n",
    "the distance from the hyperplane is important.\n",
    "\n",
    "functional margin:\n",
    "\n",
    "$$\\hat{\\gamma}^{(i)}=y^{(i)}(w^{T}x^{(i)} + b)$$\n",
    "\n",
    "geometric margin:\n",
    "\n",
    "$$\\gamma^{(i)}=\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| }$$\n",
    "\n",
    "geometric margin is the euclid distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 the optimal margin classifier\n",
    "\n",
    "we want to maximize geometic margin:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\gamma, w, b}{max}\\ &\\gamma \\\\\n",
    "s.t\\quad &\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| } >= \\gamma\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "without loss of generality, we can set $\\gamma\\left \\| w \\right \\|=1$, then the above is equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 \\\\\n",
    "s.t\\quad &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the lagrangian of this problem:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w,b,\\alpha )=\\frac{1}{2}\\left \\| w \\right \\|^{2} - \\sum_{i=1}^{n}\\alpha_{i}\\left [ y^{(i)}(w^{T}x^{(i)} + b) - 1 \\right ]     \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
