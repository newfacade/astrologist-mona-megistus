{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "given a dataset $\\left \\{ (x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}) \\right \\} $, where $x^{(i)} \\in \\mathbb{R}^{d},\\ y^{(i)} \\in \\mathbb{R}$.\n",
    "\n",
    "linear regression is trying to model $y^{(i)}$ from $x^{(i)}$ by a linear model:\n",
    "\n",
    "$$h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}x_{1}^{(i)} + \\theta_{2}x_{2}^{(i)} + ... + \\theta_{d}x_{d}^{(i)}\\approx y^{(i)}$$\n",
    "\n",
    "for the sake of simplicity, we set $x_{0}^{(i)}=1,\\ x^{(i)} = (x_{0}^{(i)},...,x_{d}^{(i)}),\\ \\theta=(\\theta_{0},...,\\theta_{d})$, then:\n",
    "\n",
    "$$h_{\\theta}(x^{(i)}) = \\sum_{j=0}^{d}\\theta_{j}x_{j}^{(i)} = \\theta^{T}x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function\n",
    "\n",
    "given $\\hat{y}=(h_{\\theta}^{(1)},...,h_{\\theta}^{(n)}),\\ y=(y^{(1)},...,y^{(n)})$.\n",
    "\n",
    "we want to approximate $y$ by $\\hat{y}$, or equivalent to say, we want to minimize the distance between $\\hat{y}$ and $y$.\n",
    "\n",
    "using euclidean distance, we derive the loss function for linear regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n}(h_{\\theta}x^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent in general\n",
    "\n",
    "after defining model and loss function, our goal now is to find the model that minimize the loss function:\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta}{argmin}\\ J(\\theta)$$\n",
    "\n",
    "to minimize the loss function, we can set \n",
    "\n",
    "$$\\nabla J(\\theta)=0$$\n",
    "\n",
    "but to find the analytic solution of this equation is usually impossible.\n",
    "\n",
    "alternatively, we can init $\\theta$ randomly, then iteratively move $\\theta$ towards the direction that makes $J(\\theta)$ smaller.\n",
    "\n",
    "remmenber the oposite direction of gradient is the fastest direction that makes functions smaller, we derive gradient descent:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\nabla{J(\\theta )}$$\n",
    "\n",
    "$\\alpha > 0$ is called the learning rate\n",
    "\n",
    "appendix: proof of opposite gradient as fastest descent\n",
    "\n",
    "for all $l\\in \\mathbb{R}^{d}, \\left \\| l \\right \\| =1$\n",
    "\n",
    "$$\\lim_{x \\to 0} \\frac{J(\\theta + x l) - J(\\theta ) }{x}=l \\cdot \\nabla{J(\\theta )} >= -\\left \\| l \\right \\|\\left \\|J(\\theta)  \\right \\| = -\\left \\|  J(\\theta )\\right \\|   $$\n",
    "\n",
    "equality obtained only if $l$ is in the opposite direction of $\\nabla{J(\\theta )}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stochastic gradient descent\n",
    "\n",
    "in practice, to update $\\theta$ by gradient descent(concretely batch gradient descent), at each step, we need to calculate all sample's gradient, too slow.\n",
    "\n",
    "to fix this problem, we can only use one sample's gradient at a time, choose that sample randomly:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha{l(h_{\\theta}(x^{(i)}), y^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch gradient descent\n",
    "\n",
    "sgd too random, batch gradient descent too slow, we can use some(not one, not all) samples at a time:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\sum_{i \\in batch}{l(h_{\\theta}(x^{(i)}), y^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update rule of linear regression\n",
    "\n",
    "for linear regression, we have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split} \n",
    "\\frac{\\partial }{\\partial \\theta_{j}}J(\\theta ) &=  \\frac{\\partial }{\\partial \\theta_{j}}\\frac{1}{2}\\sum_{i=1}^{n}(h_{\\theta }(x^{(i)}) - y^{(i)})^2  \\\\ \n",
    "&=\\sum_{i=1}^{n}(h_{\\theta }(x^{(i)}) - y^{(i)})\\cdot{}\\frac{\\partial }{\\partial \\theta_{j}}(h_{\\theta }(x^{(i)}) - y^{(i)})\\\\ \n",
    "& =\\sum_{i=1}^{n}(h_{\\theta }(x^{(i)}) - y^{(i)})x_{j}^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "so we have the updat rule for linear regression:\n",
    "\n",
    "$$\\theta_{j}: =\\theta_{j} - \\alpha\\sum_{i=1}^{n} (h_{\\theta }(x^{(i)}) - y^{(i)})x_{j}^{(i)} $$\n",
    "\n",
    "combine all dimensions, we have:\n",
    "\n",
    "$$\\theta: =\\theta - \\alpha\\sum_{i=1}^{n} (h_{\\theta }(x^{(i)}) - y^{(i)})\\cdot x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix form\n",
    "\n",
    "define $X = [(x^{(1)})^{T},...,(x^{(n)})^{T}]$, then we can write $J(\\theta)$ in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J(\\theta) &= \\frac{1}{2}\\sum_{i=1}^{n}(h_{\\theta}x^{(i)} - y^{(i)})^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{n}(\\theta^{T}x^{(i)} - y^{(i)})^2 \\\\\n",
    "&= \\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we then have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla{J(\\theta )} &= \\nabla\\frac{1}{2}(X\\theta - y)^{T}(X\\theta - y) \\\\\n",
    "&= \\frac{1}{2}\\nabla(\\theta^{T}X^{T}X\\theta - y^{T}(X\\theta) - (X\\theta)^{T}y) \\\\\n",
    "&= \\frac{1}{2}\\nabla(\\theta^{T}X^{T}X\\theta - 2(X^{T}y)^{T}\\theta) \\\\\n",
    "&= \\frac{1}{2}(2X^{T}X\\theta - 2(X^{T}y)) \\\\\n",
    "&= X^{T}X\\theta - X^{T}y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "here we use\n",
    "\n",
    "1.$a^{T}b=b^{T}a$, obvious.\n",
    "\n",
    "2.$\\nabla{a^{T}x}=a$, obvious.\n",
    "\n",
    "3.$\\nabla{x^{T}Ax} = (A + A^{T})x$, proof:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial x_{i}}x^{T}Ax &= \n",
    "\\frac{\\partial}{\\partial x_{i}}{\\sum_{j=1}^{n}}{\\sum_{k=1}^{n}}a_{jk}x_{j}x_{k} \\\\\n",
    "&= \\frac{\\partial}{\\partial x_{i}}(\\sum_{j\\ne{i}}a_{ji}x_{j}x_{i} + \\sum_{k\\ne{i}}a_{ik}x_{i}x_{k} + a_{ii}x_{i}^{2}) \\\\\n",
    "&= \\sum_{j\\ne{i}}a_{ji}x_{j} + \\sum_{k\\ne{i}}a_{ik}x_{k} + 2a_{ii}x_{ii} \\\\\n",
    "&= \\sum_{j=1}^{n}a_{ji}x_{j} + \\sum_{k=1}^{n}a_{ik}x_{k} \\\\\n",
    "&= (A^{T}x)_{i} + (Ax)_{i}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "finally, we get the matrix form of the update rule:\n",
    "\n",
    "$$\\theta: =\\theta - \\alpha X^{T}(X\\theta-\\mathbf{y} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analytic solution\n",
    "\n",
    "from above, we have:\n",
    "\n",
    "$$\\nabla{J(\\theta )} = X^{T}X\\theta - X^{T}y$$\n",
    "\n",
    "so the equation of zero gradient change to:\n",
    "\n",
    "$$X^{T}X\\theta - X^{T}y = 0$$\n",
    "\n",
    "if $X^{T}X$ is invertible:\n",
    "\n",
    "$$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "if $X^{T}X$ is not invertible, the equation also have solution. proof:\n",
    "\n",
    "on one hand $X\\theta = 0 \\Rightarrow X^{T}X\\theta=0$<br>\n",
    "on the other hand $X^{T}X\\theta=0 \\Rightarrow \\theta^{T}X^{T}X\\theta=0 \\Rightarrow (X\\theta)^{T}X\\theta=0 \\Rightarrow X\\theta=0$<br>\n",
    "so $X\\theta=0 \\Leftrightarrow X^{T}X\\theta=0$, that is to say $null(X^{T}X)=null(X)$<br>\n",
    "we easily derive from the above that $rank(X^{T}X) = rank(X) = rank(X^{T})$<br>\n",
    "but $range(X^{T}X) \\subseteq  range(X^{T})$, so we must have $range(X^{T}X)=range(X^{T})$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geometric interpretation of the linear regression\n",
    "\n",
    "consider the linear space $S = span\\left \\{  columns\\ of\\ X \\right \\}$, linear combination of $S$ could be write as $X\\theta$, then:\n",
    "\n",
    "$X\\theta$ is the projection of $y$ on $S \\Leftrightarrow$ $X\\theta - y$ orthogonal with $S \\Leftrightarrow$ orthogonal with $columns\\ of\\ X \\Leftrightarrow X^{T}(X\\theta - y)=0$\n",
    "\n",
    "so linear regression could be interpret as finding the projection of $y$ on $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularization\n",
    "\n",
    "to lower variance $\\Rightarrow $ to limit model's complexity $\\Rightarrow $ to prevent the absolute value of parameters to be too large $\\Rightarrow $ we add punishment term concerning the absolute value of parameters on $J(\\theta)$\n",
    "\n",
    "often we ignore $\\theta_{0}$ when counting model's complexity, so denote $\\bar{\\theta}:=(\\theta_{1},...,\\theta_{d})$ i.e parameters except intercept, choose $\\lambda{\\left \\| \\bar{\\theta}   \\right \\|}^{2}$ as the punishment term:\n",
    "\n",
    "$$J(\\theta) := J(\\theta) + \\lambda{\\left \\| \\bar{\\theta}   \\right \\|}^{2}$$\n",
    "\n",
    "$\\lambda$ is the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probabilistic interpretation\n",
    "\n",
    "assume targets and inputs are related via:\n",
    "\n",
    "$$y^{(i)} = \\theta^{T}x^{(i)} + \\epsilon^{(i)}$$\n",
    "\n",
    "where $\\epsilon^{(i)}$ is the error term, assume that $\\epsilon^{(i)}$ are distributed IID according to Gaussian with mean 0 and variance $\\sigma^{2}$, i.e the density of $\\epsilon^{(i)}$ is given by:\n",
    "\n",
    "$$p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left (-\\frac{(\\epsilon^{(i)})^{2}}{2\\sigma^{2}}\\right )$$\n",
    "\n",
    "this implies:\n",
    "\n",
    "$$p(y^{(i)}|x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right)$$\n",
    "\n",
    "we should denote that $\\theta$ is not a random variable.\n",
    "\n",
    "given $X$ and $\\theta$, what is the probability of $y$? we call it the likelihood function:\n",
    "\n",
    "$$L(\\theta) = L(\\theta; X,y) = p(y|X; \\theta)$$\n",
    "\n",
    "for the above assumptions:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta) &= \\prod_{i=1}^{n}p(y^{(i)}|x^{(i)}; \\theta) \\\\\n",
    "&= \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we should choose $\\theta$ so as to make the data as high probability as possible, i.e we should choose $\\theta$ to maxmize $L(\\theta)$, this is called maximum likelihood. one step further:\n",
    "\n",
    "$$maximize\\ L(\\theta) \\Leftrightarrow maxmize\\ log(L(\\theta))$$\n",
    "\n",
    "so we maximize the log likelihood, this is simpler.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta) &= log(L(\\theta)) \\\\\n",
    "&= log\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right) \\\\\n",
    "&= \\sum_{i=1}^{n}log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right) \\\\\n",
    "&= n\\ log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})^{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "hence, maximizing $l(\\theta)$ gives the same answer as minimizing\n",
    "\n",
    "$$\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})^{2} = J(\\theta)$$\n",
    "\n",
    "so linear regression $\\Leftrightarrow $ maximum likelihood given Gaussian error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
