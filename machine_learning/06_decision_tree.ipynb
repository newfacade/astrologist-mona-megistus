{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.decision tree's idea\n",
    "\n",
    "given dataset $D=\\left\\{(x^{(i)},y^{(i)})\\right\\}$\n",
    "\n",
    "decision tree is trying to pick $(feature, value)$ that partition the dataset to subsets\n",
    "\n",
    "after that partition, elements in each subsets is similar in total, i.e we gain certainty.\n",
    "\n",
    "continue the process, until we get subset that is very pure or partition too many times.\n",
    "\n",
    "we thus create a tree called the decision tree. \n",
    "\n",
    "when predicting, find leaf subset of that sample, then use typical value of leaf as prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. information gain\n",
    "\n",
    "we use entropy to measure the uncertainty of data.\n",
    "\n",
    "for classfication problem, assume $y_{(i)} \\in \\left\\{1,...,k\\right\\}$, we have the entropy of dataset $D$:\n",
    "\n",
    "$$H(D) = E(-log\\ p_{i}) = -\\sum_{i=1}^{k}p_{i}log\\ p_{i}$$\n",
    "\n",
    "$p_{i}$ is the frequency of i-th class, it defines the uncertainty of $D$.\n",
    "\n",
    "suppose we partition $D$ according to feature $A$ into $D_{1},...,D_{n}$, we have:\n",
    "\n",
    "$$H(D|A)=\\sum_{i=1}^{n}\\frac{\\#D_{i}}{\\#D}H(D_{i})$$\n",
    "\n",
    "that is: the uncertainty of $D$ after knowing $A$.\n",
    "\n",
    "information gain is uncertainty loss:\n",
    "\n",
    "$$g(D,A) = H(D) - H(D|A)$$\n",
    "\n",
    "decision tree ID3 choose feature $A$ that maximize $g(D,A)$ until:\n",
    "\n",
    "1. subset is empty\n",
    "2. information gain $g(D,A)\\le\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.information gain ratio\n",
    "\n",
    "if use information gain, we prefer feature $A$ such that $\\#A$ is large.\n",
    "\n",
    "more precisely, we prefer features that is uncertain\n",
    "\n",
    "$$H_{A}(D) =-\\sum_{i=1}^{n}\\frac{\\#D_{i}}{\\#D}log\\ \\frac{\\#D_{i}}{\\#D}$$\n",
    "\n",
    "defines that uncertainty, it is the entropy of viewing category of $A$ as labels.\n",
    "\n",
    "to fix that problem, we define the information gain ratio:\n",
    "\n",
    "$$g_{R}(D,A)=\\frac{g(D,A)}{H_{A}(D)}=\\frac{H(D)-H(D|A)}{H_{A}(D)}$$\n",
    "\n",
    "algorithm that uses $g_{R}(D,A)$ is C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. pruning\n",
    "\n",
    "we need to pruning the decision tree $\\Rightarrow $ lower model's complexity $\\Rightarrow $ mitigate overfit\n",
    "\n",
    "suppose now we have a decision tree $T$, use $\\left | T \\right | $ to denote the number of leaves of $T$, and these leaves are $T_{1},...,T_{\\left | T \\right | }$.\n",
    "\n",
    "then entropy of leaf $t$: $H(T_{t})$\n",
    "\n",
    "total entropy of these leaves:\n",
    "\n",
    "$$C(T) = \\sum_{t=1}^{\\left | T \\right |} \\left | T_{t} \\right |H(T_{t})$$\n",
    "\n",
    "we want these minimize this entropy, and at the same time limit model's complexity, give rise to the loss function:\n",
    "\n",
    "$$C_{\\alpha}(T) = C(T) + \\alpha\\left | T \\right |$$\n",
    "\n",
    "in practice, pruning is from leaves to root.\n",
    "\n",
    "if pruning a node result in a decrease of the loss function, the operate this pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CART-classification and regression tree\n",
    "\n",
    "CART can solve both the classification and regression problem.\n",
    "\n",
    "CART simply uses different strategies for them.\n",
    "\n",
    "for regression problem, we try to find feature $j$ and cutting point $s$ that minimize the square error:\n",
    "\n",
    "$$\\underset{j,s}{min}\\left[\\underset{c_{1}}{min}\\sum_{x_{i} \\in R_{1}(j, s)}(y_{i} - c_{1})^{2} + \\underset{c_{2}}{min}\\sum_{x_{i} \\in R_{2}(j, s)}(y_{i} - c_{2})^{2}\\right]$$\n",
    "\n",
    "rather than optimizing information gain or information gain ratio.\n",
    "\n",
    "CART optimize Gini-index when facing a classification problem:\n",
    "\n",
    "$$Gini(D) = E(1 - p_{i}) = \\sum_{i=1}^{k}p_{i}(1 - p_{i})$$\n",
    "\n",
    "here, rather than self-information $-log\\ p_{i}$ uses in entropy, we use $1 - p_{i}$ to indicate the information of event with probability $p_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ensemble learning and bagging\n",
    "\n",
    "ensemble learning: combine multiple weak leaners to form a strong learner.\n",
    "\n",
    "bagging is one approach of ensemble learning.\n",
    "\n",
    "it uses some randomness, e.g sampling at random, then generate a bunch of different base learner. \n",
    "\n",
    "when we say sampling at random, we mean sampling with replacement. \n",
    "\n",
    "we usually select $m$ samples with replacement from m samples.\n",
    "\n",
    "when predicting:\n",
    "\n",
    "1. if classification, use vote.\n",
    "2. if regression, use mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. random foreset\n",
    "\n",
    "random foreset is one implementation of bagging with decision tree as base learner.\n",
    "\n",
    "besides sampling at random, random foreset added some random when split.\n",
    "\n",
    "rather than using all $d$ features when split, we randomly select it's $k$ features subset, and select from this subset.\n",
    "\n",
    "often, we set $k = log_{2}d$.\n",
    "\n",
    "in one word:\n",
    "\n",
    "random foreset $=$ decision tree $+$ bagging $+$ use random subset when split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
