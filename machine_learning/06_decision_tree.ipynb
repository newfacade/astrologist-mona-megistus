{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.decision tree's idea\n",
    "\n",
    "given dataset $D=\\left\\{(x^{(i)},y^{(i)})\\right\\}$\n",
    "\n",
    "decision tree is trying to pick $(feature, value)$ that partition the dataset to subsets\n",
    "\n",
    "after that partition, elements in each subsets is similar in total, i.e we gain certainty.\n",
    "\n",
    "continue the process, until we get subset that is very pure or partition too many times.\n",
    "\n",
    "we thus create a tree called the decision tree. \n",
    "\n",
    "when predicting, find leaf subset of that sample, then use typical value of leaf as prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.entropy\n",
    "\n",
    "suppose a discrete random variable $X$, it's distribution is $p(x)$.\n",
    "\n",
    "when $X = x$, we define the self-information of that event is:\n",
    "\n",
    "$$I(x) = -log\\ p(x)$$\n",
    "\n",
    "it indicates the information in that event.\n",
    "\n",
    "we define the entropy of X as:\n",
    "\n",
    "$$H(x) = E(I(x)) = E(-log\\ p(x)) = -\\sum_{x \\in \\mathcal{X}}log\\ p(x)$$\n",
    "\n",
    "indicates the uncertainty of random variable $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. information gain\n",
    "\n",
    "for classfication problem, assume $y_{(i)} \\in \\left\\{1,...,k\\right\\}$, we have the entropy of dataset $D$:\n",
    "\n",
    "$$H(D) = -\\sum_{i=1}^{k}p_{i}log\\ p_{i}$$\n",
    "\n",
    "$p_{i}$ is the frequency of i-th class, it defines the uncertainty of $D$.\n",
    "\n",
    "suppose we partition $D$ according to feature $A$ into $D_{1},...,D_{n}$, we have:\n",
    "\n",
    "$$H(D|A)=\\sum_{i=1}^{n}\\frac{\\#D_{i}}{\\#D}H(D_{i})$$\n",
    "\n",
    "that is: the uncertainty of $D$ after knowing $A$.\n",
    "\n",
    "information gain is uncertainty loss:\n",
    "\n",
    "$$g(D,A) = H(D) - H(D|A)$$\n",
    "\n",
    "decision tree ID3 choose feature $A$ that maximize $g(D,A)$ until:\n",
    "\n",
    "1. subset is empty\n",
    "2. information gain $g(D,A)\\le\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.information gain ratio\n",
    "\n",
    "if use information gain, we prefer feature $A$ such that $\\#A$ is large.\n",
    "\n",
    "more precisely, we prefer features that is uncertain\n",
    "\n",
    "$$H_{A}(D) =-\\sum_{i=1}^{n}\\frac{\\#D_{i}}{\\#D}log\\ \\frac{\\#D_{i}}{\\#D}$$\n",
    "\n",
    "defines that uncertainty, it is the entropy of viewing category of $A$ as labels.\n",
    "\n",
    "to fix that problem, we define the information gain ratio:\n",
    "\n",
    "$$g_{R}(D,A)=\\frac{g(D,A)}{H_{A}(D)}=\\frac{H(D)-H(D|A)}{H_{A}(D)}$$\n",
    "\n",
    "algorithm that uses $g_{R}(D,A)$ is C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
