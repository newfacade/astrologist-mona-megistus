{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Mixtures of Gaussians\n",
    "\n",
    "given a training set $\\left\\{x^{(1)},...,x^{(n)}\\right\\}$ with no labels. each point $x^{(i)}$ has a latent variable $z^{(i)}$, the data is specified by a joint probability $p(x^{(i)}, z^{(i)}) = p(x^{(i)}| z^{(i)})p(z^{(i)})$\n",
    "\n",
    "mixtures of gaussians assumes $z^{(i)}\\sim Multinomial(\\phi)$ (where $\\phi_{j} \\ge 0, \\sum_{j=1}^{k}\\phi_{j}=1$, $\\phi_{j}$ gives $p(z^{(i)}=j)$), and $x^{(i)}|z^{(i)}=j \\sim \\mathcal{N}(\\mu_{j}, \\Sigma_{j})$. we let $k$ denote the number of values $z^{(i)}$'s can take on.\n",
    "\n",
    "thus our model posits that each $x^{(i)}$ was generated by randomly choosing $z^{(i)}$ from $\\{1,...,k\\}$, and $x^{(i)}$ was drawn from one of $k$ gaussians depending on $z^{(i)}$.\n",
    "\n",
    "the parameters of our model are $\\phi, \\mu, \\Sigma$, to estimate them, we can write down the likelihood of our data:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\phi, \\mu, \\Sigma) =& \\sum_{i=1}^{n}log\\,p(x^{(i)};\\phi,\\mu,\\Sigma)\\\\\n",
    "=& \\sum_{i=1}^{n}log\\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)}; \\mu,\\Sigma)p(z^{(i)};\\phi)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "it is not possible to find the maximum likelihood estimate of the parameters in closed form.\n",
    "\n",
    "on the other hand, if we knew what $z^{(i)}$'s were, the maximum likelihood problem would have been easy.\n",
    "\n",
    "specifically, we could then write down the likelihood as:\n",
    "\n",
    "$$l(\\phi, \\mu, \\Sigma) = \\sum_{i=1}^{n}log\\,p(x^{(i)}|z^{(i)};\\mu,\\Sigma) + log\\,p(z^{(i)}; \\phi)$$\n",
    "\n",
    "maximizing this with respect to $\\phi, \\mu, \\Sigma$ gives the parameters:\n",
    "\n",
    "$$\\phi_{j} = \\frac{1}{n}\\sum_{i=1}^{n}1\\{z^{(i)} = j\\}$$\n",
    "\n",
    "$$\\mu_{j} = \\frac{\\sum_{i=1}^{n}1\\{z^{(i)} = j\\}x^{(i)}}{\\sum_{i=1}^{n}1\\{z^{(i)} = j\\}}$$\n",
    "\n",
    "$$\\Sigma_{j} = \\frac{\\sum_{i=1}^{n}1\\{z^{(i)} = j\\}(x^{(i)} - \\mu_{j})(x^{(i)} - \\mu_{j})^{T}}{\\sum_{i=1}^{n}1\\{z^{(i)} = j\\}}$$\n",
    "\n",
    "it is identical to the gaussian discriminant analysis(GDA).\n",
    "\n",
    "however, in our density estimation problems, $z^{(i)}$'s are not known, what can we do?\n",
    "\n",
    "the EM algorithm is an iterative algorithm that has two main steps:\n",
    "\n",
    "**1.the E-step, it tries to \"guess\" the values of the $z^{(i)}$'s.**\n",
    "\n",
    "**2.the M-step, it updates the parameters of our model based on our guesses.**\n",
    "\n",
    "since in the M-step we are pretending that the guesses in the first part were correct, the maximization becomes easy, here's the algorithm:\n",
    "\n",
    "\n",
    "(E-step) for each $i, j$ set\n",
    "\n",
    "$$w_{j}^{(i)} := p(z^{i}=j|x^{(i)}; \\phi,\\mu,\\Sigma)$$\n",
    "\n",
    "(M-step) update the parameters:\n",
    "\n",
    "$$\\phi_{j} := \\frac{1}{n}\\sum_{i=1}^{n}w_{j}^{(i)}$$\n",
    "\n",
    "$$\\mu_{j} := \\frac{\\sum_{i=1}^{n}w_{j}^{(i)}x^{(i)}}{\\sum_{i=1}^{n}w_{j}^{(i)}}$$\n",
    "\n",
    "$$\\Sigma_{j} := \\frac{\\sum_{i=1}^{n}w_{j}^{(i)}(x^{(i)} - \\mu_{j})(x^{(i)} - \\mu_{j})^{T}}{\\sum_{i=1}^{n}w_{j}^{(i)}}$$\n",
    "\n",
    "the EM-algorithm likes the K-means-algorithm.\n",
    "\n",
    "except that instead of \"hard\" cluster assignments $c(i)$, we have the \"soft\" assignments $w_{j}^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.The EM algorithm\n",
    "\n",
    "here comes the general view of EM.\n",
    "\n",
    "like the assumption before, we have the training set $\\left\\{x^{(1)},...,x^{(n)}\\right\\}$, each point $x^{(i)}$ has a latent variable $z^{(i)}$.\n",
    "\n",
    "denote $\\theta$ as the distribution parameters($\\phi,\\mu,\\Sigma$ in mixtures of gaussian), then density of $x$ can be obtained by:\n",
    "\n",
    "$$p(x;\\theta) = \\sum_{z}p(x,z;\\theta)$$\n",
    "\n",
    "the log-likelihood of the data:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta) =& \\sum_{i=1}^{n}log\\,p(x^{(i)}; \\theta) \\\\\n",
    "=& \\sum_{i=1}^{n}log\\sum_{z^{(i)}}p(x^{(i)}, z^{(i)}; \\theta)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "instead of solving the maximum-likelihood directly(usually very hard), the EM algorithm repeatedly construct a lower-bound on $l$(E-step), and then optimize that lower-bound(M-step).\n",
    "\n",
    "we first consider optimizing the likelihood $log\\,p(x)$ for a single example $x$, optimize $log\\,p(x;\\theta)$:\n",
    "\n",
    "$$log\\,p(x;\\theta) = log\\sum_{z}p(x,z;\\theta)$$\n",
    "\n",
    "let $Q$ be a distribution over the possible values of $z$, then:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "log\\,p(x;\\theta) =& log\\sum_{z}p(x,z;\\theta)\\\\\n",
    "=& log\\sum_{z}Q(z)\\frac{p(x,z;\\theta)}{Q(z)}\\\\\n",
    "\\ge& \\sum_{z}Q(z)log\\frac{p(x,z;\\theta)}{Q(z)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the last step uses Jensen's inequality, for $(log\\,x)'' = -1/x^{2} < 0$ so that strictly concave.\n",
    "\n",
    "for any distribution $Q$, the above formula gives a lower-bound on $log\\,p(x;\\theta)$.\n",
    "\n",
    "to make the bound tight for a particular value of $\\theta$, we want the Jensen's inequality to hold with equality. it is sufficient the expecation take on \"constant\"-valued random variables, i.e:\n",
    "\n",
    "$$\\frac{p(x,z;\\theta)}{Q(z)} = c$$\n",
    "\n",
    "this leads to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Q(z) =& \\frac{p(x,z;\\theta)}{\\sum_{z}p(x,z;\\theta)}\\\\\n",
    "=& \\frac{p(x,z;\\theta)}{p(x;\\theta)}\\\\\n",
    "=& p(z|x;\\theta)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "thus, we simply set $Q$ to be the posterior distribution of $z$'s given $x$ and $\\theta$.\n",
    "\n",
    "for convenience, we call the bound as **evidence lower bound(ELBO)**:\n",
    "\n",
    "$$ELBO(x;Q,\\theta) = \\sum_{z}Q(z)log\\frac{p(x,z;\\theta)}{Q(z)}$$\n",
    "\n",
    "the estimation now can re-write as:\n",
    "\n",
    "$$\\forall Q,\\theta,x,\\quad log\\,p(x;\\theta) \\ge ELBO(x;Q,\\theta)$$\n",
    "\n",
    "Intuitively, the EM algorithm alternatively update $Q$ and $\\theta$ by:\n",
    "\n",
    "a.setting $Q(z) = p(z|x;\\theta)$ so that $ELBO(x;Q,\\theta) = log\\,p(x;\\theta)$\n",
    "\n",
    "b.maximizing $ELBO(x;Q,\\theta)$ with respect to $\\theta$.\n",
    "\n",
    "now consider multiple examples $\\left\\{x^{(1)},...,x^{(n)}\\right\\}$, note the optimal choice of $Q$ is $p(z|x;\\theta)$, and it depends on the particular example $x$.\n",
    "\n",
    "therefore we introduce $n$ distributions $Q_{1},...,Q_{n}$, one for each example $x^{(i)}$, for each example:\n",
    "\n",
    "$$log\\,p(x^{(i)};\\theta) \\ge ELBO(x^{(i)};Q_{i},\\theta) = \\sum_{z^{(i)}}Q_{i}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_{i}(z^{(i)})}$$\n",
    "\n",
    "taking sum over all examples, we obtain a lower bound for the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta) \\ge& \\sum_{i}ELBO(x^{(i)};Q_{i},\\theta)\\\\\n",
    "=& \\sum_{i}\\sum_{z^{(i)}}Q_{i}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_{i}(z^{(i)})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the above inequality holds for **any** distributions $Q_{1},...,Q_{n}$. equality holds when:\n",
    "\n",
    "$$Q_{i}(z^{(i)}) =  p(z^{(i)}|x^{(i)};\\theta)$$\n",
    "\n",
    "we now come to the definition of EM algorithm:\n",
    "\n",
    "(E-step) for each $i$, set:\n",
    "\n",
    "$$Q_{i}(z^{(i)}) :=  p(z^{(i)}|x^{(i)};\\theta)$$\n",
    "\n",
    "(M-step) for fixed $Q_{i}$'s, set:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta :=& \\underset{\\theta}{argmax}\\sum_{i=1}^{n}ELBO(x^{(i)};Q_{i},\\theta)\\\\\n",
    "=& \\underset{\\theta}{argmax}\\sum_{i=1}^{n}\\sum_{z^{(i)}}Q_{i}(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_{i}(z^{(i)})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Convergence of EM algorithm\n",
    "\n",
    "suppose $\\theta^{(t)}$ and $\\theta^{(t+1)}$ are the parameters from two successive iterations of EM, we will now prove that $l(\\theta^{(t)}) \\le l(\\theta^{(t+1)})$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta^{(t+1)}) \\ge& \\sum_{i=1}^{n}ELBO(x^{(i)};Q_{i}^{(t)};\\theta^{(t+1)})\\quad (\\mbox{by Jensen's inequality})\\\\\n",
    "\\ge& \\sum_{i=1}^{n}ELBO(x^{(i)};Q_{i}^{(t)};\\theta^{(t)})\\quad (\\mbox{by M-step}) \\\\\n",
    "=& l(\\theta^{(t)})\\quad (\\mbox{by E-step})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "if we define:\n",
    "\n",
    "$$ELBO(Q, \\theta) = \\sum_{i=1}^{n}ELBO(x^{(i)}; Q_{i}, \\theta)$$\n",
    "\n",
    "then we know $l(\\theta) \\ge ELBO(Q, \\theta)$.\n",
    "\n",
    "the EM algorithm can be viewed an alternating maximization algorithm on $ELBO(Q, \\theta)$:\n",
    "\n",
    "the E-step maximizes it with respect to $Q$.\n",
    "\n",
    "the M-steo maximizes it with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
