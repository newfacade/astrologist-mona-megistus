{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.boosting\n",
    "\n",
    "boosting is another approach of ensemble learning.\n",
    "\n",
    "boosting first train a base learner which is rather weak, then adjust that leaner with the knowledge of current prediction to make it stronger.\n",
    "\n",
    "for example, adjust the traing set distribution, focus on samples that previous predict wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.adaboost\n",
    "\n",
    "suppose a binary classification problem, dataset is $\\left\\{(x_{1}, y_{1}),...,({x_{N},y_{N}})\\right\\}$, $y_{i} \\in \\left\\{-1, 1\\right\\}$, total $N$ samples.\n",
    "\n",
    "initial sample distribution is $D_{1} = (\\frac{1}{N},...,\\frac{1}{N}) = (w_{1,1},...,w_{1,N})$\n",
    "\n",
    "for $m=1,...,M$:\n",
    "\n",
    "1. train $G_{m}: \\mathcal{X} \\rightarrow \\left\\{-1, 1\\right\\}$ based on $D_{m} = (w_{m,1},...,w_{m,N})$\n",
    "\n",
    "2. compute misclassify error:\n",
    "$$e_{m} = \\sum_{i=1}^{N}P(G_{m}(x_{i}) \\ne y_{i}) = \\sum_{i=1}^{N}w_{m,i}I(G_{m}(x_{i} \\ne y_{i}))$$\n",
    "3. update distribution $D_{m}$ to $D_{m+1}$:\n",
    "$$\n",
    "w_{m+1, i} =\n",
    "\\begin{cases}\n",
    "\\frac{w_{m,i}}{Z_{m}} \\\\\n",
    "{\\frac{1 - e_{m}}{e_{m}}}\\frac{w_{m,i}}{Z_{m}}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $Z_{m}$ is the normalization factor that makes $D_{m+1}$ a proper distribution.<br>\n",
    "this step simply enlarge the previous wrong predicted sample's distribution by a factor $\\frac{1 - e_{m}}{e_{m}}$.\n",
    "\n",
    "then construct a linear combination of $G_{m}$:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}log\\ \\frac{1 - e_{m}}{e_{m}}G_{m}(x)$$\n",
    "\n",
    "finally the resulting classifier:\n",
    "\n",
    "$$G(x) = sign(f(x)) = sign\\left(\\sum_{i=1}^{N}log\\ \\frac{1 - e_{m}}{e_{m}}G_{m}(x)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.forward stage wise algorithm\n",
    "\n",
    "consider addictive model:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})$$\n",
    "\n",
    "here $b$ is the base model, $\\gamma_{m}$ is it's paramter.\n",
    "\n",
    "given loss function $L$, we try to solve the following optimization problem:\n",
    "\n",
    "$$\\underset{\\beta, \\gamma}{min}\\sum_{i=1}^{N}L\\left(y_{i}, \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})\\right)$$\n",
    "\n",
    "this is usually very complex.\n",
    "\n",
    "foward stage wise algorithm simplify it by going only one stage at a time:\n",
    "\n",
    "init $f_{0}(x) = 0$.\n",
    "\n",
    "for $m=1,...,M$:\n",
    "\n",
    "1. do the following one step optimization:\n",
    "$$(\\beta_{m}, \\gamma_{m}) = \\underset{\\beta,\\gamma}{argmin}\\sum_{i=1}^{N}L(y_{i}, f_{m-1}(x_{i}) + \\beta{b(x_{i},\\gamma)})$$\n",
    "2. update:\n",
    "$$f_{m}(x) = f_{m-1}(x) + \\beta_{m}b(x;\\gamma_{m})$$\n",
    "\n",
    "finally the resulting addictive model:\n",
    "\n",
    "$$f(x) = f_{M}(x) = \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})$$\n",
    "\n",
    "adaboost $\\Leftrightarrow$ foward stage wise algorithm when $L(y, f(x)) = exp(-yf(x))$, t.b.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.boosting tree\n",
    "\n",
    "boosting tree model:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}T(x; \\theta_{m})$$\n",
    "\n",
    "where $T(x; \\theta_{m})$ is a decision tree.\n",
    "\n",
    "boosting tree use the forward stage wise algorithm.\n",
    "\n",
    "for binary classification boosting tree, we use $L(y, f(x)) = exp(-yf(x))$, so equivalent to adaboost where base model is decision tree.\n",
    "\n",
    "for regression problem, we use $L(y, f(x)) = (y - f(x))^2$, i.e square error.\n",
    "\n",
    "then the forward stage wise algorithm:\n",
    "\n",
    "$$\\hat\\theta_{m} = \\underset{\\theta_{m}}{argmin}\\sum_{i=1}^{N}L(y_{i}; f_{m-1}(x) + T(x_{i};\\theta_{m}))$$\n",
    "\n",
    "$$L(y_{i}; f_{m-1}(x) + T(x_{i};\\theta_{m})) = [y - f_{m-1}(x) - T(x;\\theta_{m})]^{2} = [r - T(x;\\theta_{m})]^{2}$$\n",
    "\n",
    "here $r = y - f_{m-1}(x)$ is the residual.\n",
    "\n",
    "so for regression tree, we just need to fit the residual!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.GBDT-gradient boosting decision tree\n",
    "\n",
    "while we can easily proceed in forward stage wise algorithm if loss function is square or exponential, for generic loss, this is not easy.\n",
    "\n",
    "we use the gradient of the loss function to proceed  in generic case, that is GBDT.\n",
    "\n",
    "GBDT for regression:\n",
    "\n",
    "input: training set $T = \\left\\{(x_{1},y_{1}),...,(x_{N},y_{N})\\right\\}$, $x_{i} \\in \\mathcal{X} \\in \\mathbb{R}^{n}$, $y_{i} \\in \\mathcal{Y} \\in \\mathbb{R}$, loss function $L(y, f(x))$.\n",
    "\n",
    "output: regression tree $\\hat{f}(x)$.\n",
    "\n",
    "init \n",
    "$$f_{0}(x) = \\underset{c}{argmin}\\sum_{i=1}^{N}L(y_{i}, c)$$\n",
    "\n",
    "for $m=1,...,M$.\n",
    "\n",
    "1. for $i=1,...,N$, compute\n",
    "$$r_{mi} = -\\left[\\frac{\\partial{L(y_{i}, f(x_{i}))}}{\\partial f(x_{i})}\\right]_{f = f_{m-1}}$$\n",
    "\n",
    "2. generate a tree that fits $r_{mi}$, denote it's leaf nodes areas as $R_{mi},j=1,...,J$.\n",
    "\n",
    "3. for $c=1,...,J$, compute\n",
    "$$c_{mj} = \\underset{c}{argmin}\\sum_{x_{i} \\in R_{mj}}L(y_{i}, f_{m-1}(x_{i}) + c)$$\n",
    "\n",
    "4. update $f_{m}(x) = f_{m-1}(x) + \\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$.\n",
    "\n",
    "finally the resulting regression tree:\n",
    "\n",
    "$$\\hat{f}(x) = f_{M}(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$$\n",
    "\n",
    "in one word, while we want to minimize $L(y_{i}, f_{m-1}(x_{i}) + c)$, we just pick it's negative gradient like gradient descent:\n",
    "\n",
    "$$c = -\\left[\\frac{\\partial{L(y_{i}, f(x_{i}))}}{\\partial f(x_{i})}\\right]_{f = f_{m-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
