{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.neuron\n",
    "\n",
    "a neuron takes input $x \\in \\mathbb{R}^{d}$, multiply $x$ by weights $w$ and add bias term $b$, finally use a activation function $g$.\n",
    "\n",
    "that is:\n",
    "\n",
    "$$f(x) = g(w^{T}x + b)$$\n",
    "\n",
    "it is analogous to the functionality of biological neuron.\n",
    "\n",
    "![jupyter](./neuron.svg)\n",
    "\n",
    "some useful activation function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{sigmoid:}\\quad &g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\text{tanh:}\\quad &g(z) = \\frac{e^{z}-e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    "\\text{relu:}\\quad &g(z) = max(z,0) \\\\\n",
    "\\text{leaky relu:}\\quad &g(z) = max(z, \\epsilon{z})\\ ,\\ \\epsilon\\text{ is a small positive number}\\\\\n",
    "\\text{identity:}\\quad &g(z) = z\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "linear regression's forward process is a neuron with identity activation function.\n",
    "\n",
    "logistic regression's forward process is a neuron with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.neural network\n",
    "\n",
    "building neural network is analogous to lego bricks: you take individual bricks and stack them together to build complex structures.\n",
    "\n",
    "![jupyter](./mlp.svg)\n",
    "\n",
    "we use bracket to denote layer, we take the above as example\n",
    "\n",
    "$[0]$ denote input layer, $[1]$ denote hidden layer, $[2]$ denote output layer\n",
    "\n",
    "$a^{[l]}$ denote the output of layer $l$, set $a^{[0]} := x$\n",
    "\n",
    "$z^{[l]}$ denote the affine result of layer $l$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "where $W^{[l]} \\in \\mathbb{R}^{d[l] \\times d[l-1]}$, $b^{[l]} \\in \\mathbb{R}^{d[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.weight decay\n",
    "\n",
    "recall that to mitigate overfitting, we use $l_{2}$ and $l_{1}$ regularization in linear and logistic regression.\n",
    "\n",
    "weight decay is a alias of $l_{2}$ regularization, can be generalize to neural network, we concatenate $W^{[l]}$ and flatten it to get $w$ in this setting.\n",
    "\n",
    "first adding $l_{2}$ norm penalty:\n",
    "\n",
    "$$J(w,b) = \\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)}) + \\frac{\\lambda}{2}\\left \\|  w \\right \\|^{2} $$\n",
    "\n",
    "then by gradient descent, we have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w:=& w-\\eta\\frac{\\partial}{\\partial w}J(w, b) \\\\\n",
    "=& w-\\eta\\frac{\\partial}{\\partial w}\\left(\\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)}) + \\frac{\\lambda}{2}\\left \\|  w \\right \\|^{2}\\right) \\\\\n",
    "=& (1 - \\eta\\lambda)w - \\eta\\frac{\\partial}{\\partial w}\\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "multiply by $(1 - \\eta\\lambda)$ is weight decay.\n",
    "\n",
    "often we do not calculate bias term in regularization, so does weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.dropout\n",
    "\n",
    "to strength robustness through perturbation, we can deliberately add perturbation in traning, dropout is one of that skill.\n",
    "\n",
    "we actually do the following in hidden neuron:\n",
    "\n",
    "$$\n",
    "a_{dropout} = \n",
    "\\begin{cases}\n",
    "0 &\\text{with probability }p \\\\\n",
    "\\frac{a}{1-p} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "this operation randomly dropout neuron with probability $p$ and keep the expectation unchanged:\n",
    "\n",
    "$$E(a_{dropout}) = E(a)$$\n",
    "\n",
    "depict this process below:\n",
    "\n",
    "![jupyter](./dropout2.svg)\n",
    "\n",
    "one more thing: we do not use dropout in predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.xavier initialization\n",
    "\n",
    "to mitigate vanishing and exploding gradient, to insure breaking symmtry, we should carefully initialize weights.\n",
    "\n",
    "consider a fully connected layer without bias term and activation function:\n",
    "\n",
    "$$o_{i} = \\sum_{j=1}^{n_{in}}w_{ij}x_{j}$$\n",
    "\n",
    "suppose $w_{ij}$ draw from a distribution of 0 mean and $\\sigma^{2}$ variance, not necessarily guassian.\n",
    "\n",
    "suppose $x_{j}$ draw from a distribution of 0 mean and $\\gamma^{2}$ variance, all $w_{ij}, x_{j}$ are independent.\n",
    "\n",
    "then mean of $o_{i}$ is of course 0, variance:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Var[o_{i}] =& E[o_{i}^{2}] - (E[o_{i}])^{2}\\\\\n",
    "=&\\sum_{j=1}^{n_{in}}E[w_{ij}^{2}x_{j}^{2}] \\\\\n",
    "=&\\sum_{j=1}^{n_{in}}E[w_{ij}^{2}]E[x_{j}^{2}] \\\\\n",
    "=&n_{in}\\sigma^{2}\\gamma^{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "to keep variance fixed, we need to set $n_{in}\\sigma^{2}=1$.\n",
    "\n",
    "consider backpropagation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.backpropagation\n",
    "\n",
    "recall in forward-propagation, we have:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "consider a $L$ layer network, then prediction is obtain through:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g^{[L]}(W^{[L]}a[L-1] + b^{[L]})\n",
    "$$\n",
    "\n",
    "loss function:\n",
    "\n",
    "$$L(\\hat{y},y)$$\n",
    "\n",
    "for any given layer index $l$, we update layer $l$'s parameter by gradient descent:\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{W^{[l]}}}$$\n",
    "\n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{b^{[l]}}}$$\n",
    "\n",
    "to proceed, we must compute the gradient with respect to the parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
