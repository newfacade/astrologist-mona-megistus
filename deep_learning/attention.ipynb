{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Encoder-Decoder\n",
    "\n",
    "for problem like machine translation, we often use the encoder-decoder architecture:\n",
    "\n",
    "![jupyter](./encoder-decoder.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Seq2Seq\n",
    "\n",
    "combine rnn & encoder-decoder, we derive the seq2seq model.\n",
    "\n",
    "it behave differently in training & predicting, seq2seq in training, this is the so called teacher forcing:\n",
    "![jupyter](seq_new_train.svg)\n",
    "here red rectangle represent encoder rnn, yellow rectangle represent decoder rnn, rnn here can be simple-rnn or gru or lstm e.t.c\n",
    "\n",
    "seq2seq in predicting:\n",
    "![jupyter](seq_predict.svg)\n",
    "in machine-translation, raw text is represented by one-hot vector, before feeding in to encoder&decoder, we need to embedding them. \n",
    "\n",
    "after decoder-output, we also need a fully-connected-network to transform embedding vectors back to raw vectors.\n",
    "\n",
    "multi-layer rnns can also be applied, adding all these:\n",
    "\n",
    "![jupyter](seq2seq-details.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedded_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        embedded = self.embedding(enc_x)\n",
    "        output, state = self.rnn(embedded)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.soft_max = nn.Softmax()\n",
    "\n",
    "    def forward(self, dec_x, state):\n",
    "        embedded = self.embedding(dec_x)\n",
    "        output, state = self.rnn(embedded, state)\n",
    "        prediction = self.soft_max(self.linear(output))\n",
    "        return prediction, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Seq2SeqEncoder, decoder: Seq2SeqDecoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        _, state = self.encoder(enc_x)\n",
    "        prediction, state = self.decoder(dec_x, state)\n",
    "        return prediction, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Attention mechanism\n",
    "\n",
    "Using the nonvolitional cue based on saliency (red cup, non-paper), attention is involuntarily directed to the coffee:\n",
    "\n",
    "![jupyter](eye-coffee.svg)\n",
    "\n",
    "\n",
    "Using the volitional cue (want to read a book) that is task-dependent, attention is directed to the book under volitional control.\n",
    "\n",
    "![jupyter](eye-book.svg)\n",
    "\n",
    "Inspired by the above biology experiment, we come to the attention mechanism.\n",
    "\n",
    "1. nonvolitional cues $\\rightarrow$ keys\n",
    "2. volitional cue $\\rightarrow$ query\n",
    "3. sensory inputs $\\rightarrow$ values\n",
    "\n",
    "Attention mechanisms bias selection over values (sensory inputs) via attention pooling, which incorporates queries (volitional cues) and keys (nonvolitional cues):\n",
    "\n",
    "![jupyter](qkv.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. attention pooling: nadaraya-watson regression\n",
    "\n",
    "given a dataset $\\left\\{(x_{1},y_{1}),...,(x_{n},y_{n})\\right\\}$, where $x_{i} \\in \\mathbb{R}, y_{i} \\in \\mathbb{R}$, how to learn $f$ to predict the output $\\hat{y}=f(x)$ for any new input $x$?\n",
    "\n",
    "the \"dumbest\" estimator for this regression problem:\n",
    "\n",
    "$$f(x)= \\frac{1}{n}\\sum_{i=1}^{n}y_{i}$$\n",
    "\n",
    "![jupyter](nadaraya-waston-1.svg)\n",
    "\n",
    "nadaraya-watson regression evaluate the connection between $x$ and $x_{i}$ by a positive function $k(x,x_{i})$, then uses weighted average:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{n}\\frac{k(x, x_{i})}{\\sum_{j=1}^{n}k(x, x_{j})}y_{i}$$\n",
    "\n",
    "to illustrate, just consider guassian kernel:\n",
    "\n",
    "$$k(x, x_{i}) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(x - x_{i})^{2})$$\n",
    "\n",
    "in this setting:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{n}\\frac{exp(-\\frac{1}{2}(x - x_{i})^{2})}{\\sum_{i=1}^{n}exp(-\\frac{1}{2}(x - x_{i})^{2})}y_{i} = \\sum_{i=1}^{n}softmax\\left(-\\frac{1}{2}(x - x_{i})^{2}\\right)y_{i}$$\n",
    "\n",
    "![jupyter](nadaraya-waston-2.svg)\n",
    "\n",
    "parametric nadaraya-watson uses a learnable parameter $w$:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{n}\\frac{exp(-\\frac{1}{2}((x - x_{i})w)^{2})}{\\sum_{i=1}^{n}exp(-\\frac{1}{2}((x - x_{i})w)^{2})}y_{i} = \\sum_{i=1}^{n}softmax\\left(-\\frac{1}{2}((x - x_{i})w)^{2}\\right)y_{i}$$\n",
    "\n",
    "![jupyter](nadaraya-waston-3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.attention scoring function\n",
    "\n",
    "we can formalize nadaraya-watson regression as:\n",
    "\n",
    "1. compute query-key weight: $a(x, x_{i}) = -\\frac{1}{2}(x - x_{i})^{2}$\n",
    "2. softmax these weight, derive a distribution: $w(x, x_{i}) = softmax(a(x, x_{i}))$\n",
    "3. output weighted average: $f(x) = \\sum_{i=1}^{n}w(x, x_{i})y_{i}$\n",
    "\n",
    "this is exactly one type of attention mechanism:\n",
    "\n",
    "![jupyter](attention-output.svg)\n",
    "\n",
    "when queries and keys are vectors of different length, we can use the additive attention:\n",
    "\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = w_{h}^{T}tanh(W_{q}\\mathbf{q} + W_{k}\\mathbf{k})$$\n",
    "\n",
    "here $\\mathbf{q} \\in \\mathbb{R}^{q}, \\mathbf{k} \\in \\mathbb{R}^{k}$, learnable parameters $W_{q} \\in \\mathbb{R}^{h\\times{q}}, W_{k} \\in \\mathbb{R}^{h\\times{k}}, w_{h} \\in \\mathbb{R}^{h}$.\n",
    "\n",
    "here hidden size $h$ is a hyperparameter.\n",
    "\n",
    "it is equal to concatenate query and key, then fed into a two layer MLP with hidden size $h$ and output size 1.\n",
    "\n",
    "when query and key are of the same dimension, we can use the scaled dot-product attention:\n",
    "\n",
    "$$a(q, k) = \\frac{q^{T}k}{\\sqrt{d}}$$\n",
    "\n",
    "here $q, k \\in \\mathbb{R}^{d}$, we divide $\\sqrt{d}$ to preserve variance.\n",
    "\n",
    "suppose $q, k$ elements are independent with zero mean and unit variance, then $q^{T}k$ has zero mean and $d$ variance.\n",
    "\n",
    "scaled dot-product is more computationally efficient, furthermore, in practice, we often think in minibatches for efficiency.\n",
    "\n",
    "suppose we compute attention for $n$ queries and $m$ key-value pairs, then now the scaled dot-product:\n",
    "\n",
    "$$softmax\\left(\\frac{QK^{T}}{\\sqrt{d}}\\right)V \\in \\mathbb{R}^{n\\times{v}}$$\n",
    "\n",
    "here $Q \\in \\mathbb{R}^{n\\times{d}}, K \\in \\mathbb{R}^{m\\times{d}}, V\\in \\mathbb{R}^{m\\times{v}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.bahdanau attention\n",
    "\n",
    "review the vanilla RNN encoder-decoder:\n",
    "\n",
    "![jupyter](seq2seq1.svg)\n",
    "\n",
    "where the context variable $c$ is the same in all decoder steps.\n",
    "\n",
    "while bahdanau attention uses $c_{t^{'}}$ to replace $c$:\n",
    "\n",
    "$$c_{t^{'}} = \\sum_{i=1}^{T}\\alpha(s_{t^{'}-1}, h_{t})h_{t}$$\n",
    "\n",
    "here $T$ is the input sequence length.\n",
    "\n",
    "the decoder hidden state $s_{t^{'}-1}$ at time step $t^{'}-1$ is the query.\n",
    "\n",
    "the encoder hidden state $h_{t}$ are both keys and values.\n",
    "\n",
    "we uses additive attention here, i.e:\n",
    "\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = w_{h}^{T}tanh(W_{q}\\mathbf{q} + W_{k}\\mathbf{k})$$\n",
    "\n",
    "bahdanau attention's data-flow:\n",
    "\n",
    "![jupyter](seq2seq2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.multi-head attention\n",
    "\n",
    "given the same set of queries, keys and values, we want our model to combine knowledge from different behaviors.\n",
    "\n",
    "e.g shorter-range-dependencis vs long-range-dependencies.\n",
    "\n",
    "in multi-head attention, queries, keys and values first transformed with $h$ linear projections.\n",
    "\n",
    "each output then fed into a attention pooling, finally concatenate and followed by another fully-connected layer:\n",
    "\n",
    "![jupyter](multi-head-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. self-attention and positional encoding\n",
    "\n",
    "if queries $=$ keys $=$ values, we call the attention self-attention.\n",
    "\n",
    "suppose a sequence $x_{1}, ..., x_{n}$ where $x_{i} \\in \\mathbb{R}^{d}$, then self-attention output:\n",
    "\n",
    "$$y_{i} = f(x_{i}, (x_{1}, x_{1}),...,(x_{n}, x_{n})) \\in \\mathbb{R}^{d}$$\n",
    "\n",
    "comparing cnns, rnns and self-attention in mapping a sequences of $n$ tokens to another sequence of $n$ tokens:\n",
    "\n",
    "![jupyter](cnn-rnn-self-attention.svg)\n",
    "\n",
    "unlike rnns, self-attention ditches sequential operation in favor of parallel computation.\n",
    "\n",
    "to use sequence order information, we can inject positional information by adding positional encoding.\n",
    "\n",
    "positional encoding can be either learned or fixed, we introduce fixed positional encoding here.\n",
    "\n",
    "suppose input $X \\in \\mathbb{R}^{n\\times{d}}$ contains $d$ dimensional embedding for $n$ tokens of sequence.\n",
    "\n",
    "the positional encoding outputs $X+P$, here $P \\in \\mathbb{R}^{n\\times{d}}$, whose elements on $i^{th}$ row and $(2j)^{th}$ or $(2j+1)^{th}$ column is:\n",
    "\n",
    "$$p_{i, 2j} = sin\\left(\\frac{i}{10000^{2j/d}}\\right)$$\n",
    "\n",
    "$$p_{i, 2j + 1} = cos\\left(\\frac{i}{10000^{2j/d}}\\right)$$\n",
    "\n",
    "rows correspond to positions within a sequence, columns represent different positional encoding dimension. \n",
    "\n",
    "![jupyter](self-attention1.svg)\n",
    "\n",
    "in binary representations, a higher bit has a lower frequency than lower bits.\n",
    "\n",
    "the positional encoding decreases frequecies along the encoding dimension, as demenstrated below:\n",
    "\n",
    "![jupyter](self-attention2.svg)\n",
    "\n",
    "besides capturing absolute positional information, the above encoding also allows a model to easily learn to attend by relative positions.\n",
    "\n",
    "denoting $\\omega_{j} = 10000^{2j/d}$, then $(p_{i, 2j}, p_{i, 2j+1})$ can be linearly projected to $(p_{i + \\delta, 2j}, p_{i + \\delta, 2j+1})$ for any fixed offset $\\delta$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\begin{bmatrix}\n",
    " cos(\\delta\\omega_{j}) &sin(\\delta\\omega_{j}) \\\\\n",
    " -sin(\\delta\\omega_{j}) &cos(\\delta\\omega_{j})\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " p_{i, 2j}\\\\\n",
    "p_{i, 2j+1}\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    "cos(\\delta\\omega_{j})sin(i\\omega_{j}) + sin(\\delta\\omega_{j})cos(i\\omega_{j})\\\\\n",
    "-sin(\\delta\\omega_{j})sin(i\\omega_{j}) + cos(\\delta\\omega_{j})cos(i\\omega_{j})\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    " sin((i+\\delta)\\omega_{j})\\\\\n",
    "cos((i+\\delta)\\omega_{j})\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    " p_{i, 2j+\\delta}\\\\\n",
    "p_{i, 2j+1\\delta}\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "indepent of $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.transformer\n",
    "\n",
    "the transformer architecture:\n",
    "\n",
    "for the attention of the decoder's second-sublayer, keys and values are from the previous encoding layer outputs.\n",
    "\n",
    "![jupyter](transformer.svg)\n",
    "\n",
    "transformer is an instance of the encoder-decoder architecture, multi-head attention and positional encoding is as above.\n",
    "\n",
    "positionwise feed-forward network transforms the representation at all the sequence position using the same MLP.\n",
    "\n",
    "may be the name \"position-free\" is more appropriate.\n",
    "\n",
    "add & norm layer is a residual connection immediately followed by layer normalization.\n",
    "\n",
    "layer-normalization: accross layer.\n",
    "\n",
    "batch-normalization: accross batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
