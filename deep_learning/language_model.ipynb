{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical preprocess steps:\n",
    "\n",
    "    1.Load text as strings into memory, with some re.sub.\n",
    "\n",
    "    2.Split strings into tokens (e.g., words and characters).\n",
    "\n",
    "    3.Build a table of vocabulary to map the split tokens to numerical indices.\n",
    "\n",
    "    4.Convert text into sequences of numerical indices so they can be manipulated by models easily.\n",
    "    \n",
    "    5.Form data iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_novels():\n",
    "    \"\"\"\n",
    "    read lines of 10 famous novels.\n",
    "    basic re.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    folder_path = \"../data/novels\"\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.startswith(\".\"):\n",
    "            lines += open(os.path.join(folder_path, file), \"r\").readlines()\n",
    "    lines = [re.sub('[^A-Za-z]', ' ', line).strip().lower() for line in lines]\n",
    "    return [line for line in lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_novels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='char'):\n",
    "    \"\"\"\n",
    "    Split text lines into word or character tokens.\n",
    "    we focus on char now\n",
    "    \"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 bulid vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string type of the token is inconvenient to be used by models, which take numerical inputs. \n",
    "\n",
    "Now let us build a dictionary, often called vocabulary as well, to map string tokens into numerical indices starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    \"\"\"\n",
    "    Count token frequencies.\n",
    "    \"\"\"\n",
    "    return collections.Counter([token for line in tokens for token in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Vocabulary for text.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens may be list, tuple, string\n",
    "        \"\"\"\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"\n",
    "        indexs may be list, tuple, int \n",
    "        \"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 putting all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_novels():\n",
    "    \"\"\"\n",
    "    Return token indices and the vocabulary of the novels dataset.\n",
    "    \"\"\"\n",
    "    lines = read_novels()\n",
    "    tokens = tokenize(lines)\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line if vocab[token] != 0]\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus_novels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410043, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 form data iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter(corpus, batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    Generate a minibatch\n",
    "    result shape: (batch_size, num_steps)\n",
    "    \"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens]).reshape(batch_size, -1)\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]).reshape(batch_size, -1)\n",
    "    \n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    \"\"\"\n",
    "    An iterator to load sequence data.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, num_steps):\n",
    "        self.corpus, self.vocab = load_corpus_novels()\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "        self.data_iter_fn = seq_data_iter\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_novels(batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    Return the iterator and the vocabulary of the novel dataset.\n",
    "    \"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 40\n",
    "train_iter, vocab = load_data_novels(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.basic language model\n",
    "\n",
    "assume that the tokens in a text sequence of length  $T$  are in turn  $x_{1},x_{2},...,x_{T}$\n",
    "\n",
    "the goal of language model is to estimate the joint-probability of the sequence:\n",
    "\n",
    "$$P(x_{1},p_{2},...,x_{T})$$\n",
    "\n",
    "an ideal language model would be able to generate natural text just on its own, simply by drawing one token at a time:\n",
    "\n",
    "$$x_{t} \\sim P(x_{t}|x_{1},...,x_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 n-gram\n",
    "\n",
    "how to compute the joint-probabilty? let us start by applying basic probability rules:\n",
    "\n",
    "$$P(x_{1},...,x_{T}) = \\prod_{t=1}^{T}P(x_{t}|x_{1},...,x_{t-1})$$\n",
    "\n",
    "probability could be estimated by frequency, for example:\n",
    "\n",
    "$$\\hat{P}(learning|deep) = \\frac{n(deep, learning)}{n(deep)}$$\n",
    "\n",
    "addtionally, we commonly perform laplace smoothing:\n",
    "\n",
    "$$\\hat{P(x)} = \\frac{n(x) + \\epsilon_{1}/m}{n + \\epsilon_{1}}$$\n",
    "\n",
    "$$\\hat{P(x'|x)}=\\frac{n(x,x') + \\epsilon_{2}p(x')}{n(x) + \\epsilon_{2}}$$\n",
    "\n",
    "$$\\hat{P(x''|x,x')}=\\frac{n(x, x', x'') + \\epsilon_{3}p(x'')}{n(x, x') + \\epsilon_{3}}$$\n",
    "\n",
    "with markov assumption, we get n-gram:\n",
    "\n",
    "$$P(x_{t}|x_{1},...,x_{t-1}) = P(x_{t}|x_{t-n+1},...,x_{t-1})$$\n",
    "\n",
    "so for unigram, bigram, trigram, we each have:\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2})P(x_{3})p(x_{4})$$\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{2})p(x_{4}|x_{3})$$\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2})p(x_{4}|x_{2},x_{3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 perplexity\n",
    "\n",
    "how to measure the language model quality?\n",
    "\n",
    "we can ask about predicting the next token given the current set of tokens. A better language model should allow us to predict the next token more accurately\n",
    "\n",
    "thus can measure it by the cross-entropy loss averaged over all the  $n$  tokens of a sequence:\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{t=1}^{n}-log\\ P(x_{t}|x_{t-1},...,x_{1})$$\n",
    "\n",
    "where  𝑃  is given by a language model and  $x_{t}$  is the actual token observed at time step  $t$  from the sequence. \n",
    "\n",
    "This makes the performance on documents of different lengths comparable.\n",
    "\n",
    "For historical reasons, we use exponentials called perplexity:\n",
    "\n",
    "$$exp\\left (\\frac{1}{n}\\sum_{t=1}^{n}-log\\ P(x_{t}|x_{t-1},...,x_{1})\\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. rnn model\n",
    "\n",
    "in n-gram models, the conditional probability of a word $x_{t}$ only depends on the $n - 1$ previous words.\n",
    "\n",
    "if we want to incorporate the possible effect of words earlier than time step $t - (n - 1)$, we need to increase $n$\n",
    ". but the number of model parameters $\\left | V \\right | ^{n}$ increase exponentially\n",
    "\n",
    "Hence, rather than modeling  $P(x_{t}|x_{t-n+1},...,x_{t-1})$,  it is preferable to use a latent variable model:\n",
    "\n",
    "$$P(x_{t}|x_{1},...,x_{t-1}) \\approx P(x_{t}|h_{t-1})$$\n",
    "\n",
    "normal mlp with one hidden layer:\n",
    "\n",
    "$$H = \\phi(XW_{xh} + b_{h})$$\n",
    "\n",
    "$$O = HW_{hq} + b_{q}$$\n",
    "\n",
    "for sequence problem, Assume that we have a minibatch of inputs  $X_{t}\\in \\mathbb{R}^{n\\times{d}}$  at time step $t$, then use previous latent variable:\n",
    "\n",
    "$$H_{t} = \\phi(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})$$\n",
    "\n",
    "$$O_{t} = H_{t}W_{hq} + b_{q}$$\n",
    "\n",
    "often $\\phi = tanh$ as default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-based Character-Level Language Models:\n",
    "\n",
    "![jupyter](./rnn-train.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN descrption:\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$$h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})$$\n",
    "\n",
    "here $h_t$ is the hidden state at time $t$.<br>\n",
    "$x_t$ is the input at time $t$.<br>\n",
    "$h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # (batch_size, num_steps) to (num_steps, batch_size, vocab_size)\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size).type(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            return torch.zeros((1, batch_size, self.num_hiddens))\n",
    "        else:\n",
    "            # lstm has two states\n",
    "            return (torch.zeros((1, batch_size, self.num_hiddens)),\n",
    "                   torch.zeros((1, batch_size, self.num_hiddens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 predict & train functions\n",
    "\n",
    "predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab):\n",
    "    \"\"\"\n",
    "    Generate new characters following the prefix.\n",
    "    \"\"\"\n",
    "    state = net.begin_state(batch_size=1) # for single predict, use batch_size=1, net need to implement\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.reshape(torch.tensor([outputs[-1]]), (1, 1)) # batch_size, num_steps = 1, 1\n",
    "    # warm-up period\n",
    "    for y in prefix[1:]:\n",
    "        _, state = net(get_input(), state) # call of net\n",
    "        outputs.append(vocab[y])\n",
    "    # predict num_preds steps\n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_input(), state)\n",
    "        # y shape: (1*1, vocab_size)\n",
    "        outputs.append(int(y.argmax(dim=1))) # output max-probability\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller xx xx xx x'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train epoch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net, train_iter, loss, updater):\n",
    "    \"\"\"\n",
    "    Train a net within one epoch\n",
    "    \"\"\"\n",
    "    state = None\n",
    "    metric = [0.0, 0.0] # Sum of training loss, no. of tokens\n",
    "    for X, Y in train_iter: # shape: (batch_size, num_steps)\n",
    "        if state is None:\n",
    "            # Initialize state when it is the first iteration\n",
    "            state = net.begin_state(batch_size=X.shape[0])\n",
    "        else:\n",
    "            if isinstance(state, tuple): # lstm has two states\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "            else:\n",
    "                state.detach_()\n",
    "        y = Y.T.reshape(-1) # shape: batch_size * num_steps\n",
    "        y_hat, state = net(X, state) # y_hat shape: (num_steps * batch_size, vocab_size)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # Since the mean function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        metric[0] += float(l * len(y))\n",
    "        metric[1] += len(y)\n",
    "        # print(metric)\n",
    "    print(metric)\n",
    "    return metric[0] / metric[1] # perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs):\n",
    "    \"\"\"\n",
    "    Train a model\n",
    "    \"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # Initialize\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 30, net, vocab)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch_ch8(net, train_iter, loss, updater)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"epoch: {epoch + 1}, log ppl: {ppl}, predict: {predict('time traveller')}\")\n",
    "    print(f'log perplexity {ppl:.1f}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1133096.4404296875, 409600.0]\n",
      "[1054988.9448242188, 409600.0]\n",
      "[1000192.8452148438, 409600.0]\n",
      "[969105.1059570312, 409600.0]\n",
      "[950116.5832519531, 409600.0]\n",
      "[936716.408203125, 409600.0]\n",
      "[925909.9848632812, 409600.0]\n",
      "[916766.8342285156, 409600.0]\n",
      "[907932.7380371094, 409600.0]\n",
      "[899192.279296875, 409600.0]\n",
      "epoch: 10, log ppl: 2.195293650627136, predict: time traveller    and the  and the  and the \n",
      "[890760.8466796875, 409600.0]\n",
      "[882635.9470214844, 409600.0]\n",
      "[874675.4450683594, 409600.0]\n",
      "[867071.2788085938, 409600.0]\n",
      "[860329.0942382812, 409600.0]\n",
      "[853209.9924316406, 409600.0]\n",
      "[846818.1467285156, 409600.0]\n",
      "[840788.4501953125, 409600.0]\n",
      "[834735.4404296875, 409600.0]\n",
      "[828678.6572265625, 409600.0]\n",
      "epoch: 20, log ppl: 2.023141252994537, predict: time traveller    and the was the was the wa\n",
      "[823315.140625, 409600.0]\n",
      "[817850.8608398438, 409600.0]\n",
      "[812675.3845214844, 409600.0]\n",
      "[807695.9619140625, 409600.0]\n",
      "[802583.1977539062, 409600.0]\n",
      "[798355.2785644531, 409600.0]\n",
      "[793553.59765625, 409600.0]\n",
      "[788857.0083007812, 409600.0]\n",
      "[784880.0783691406, 409600.0]\n",
      "[780507.6040039062, 409600.0]\n",
      "epoch: 30, log ppl: 1.9055361425876618, predict: time traveller    and the sure   she was the\n",
      "[776122.5729980469, 409600.0]\n",
      "[772213.8903808594, 409600.0]\n",
      "[768309.9702148438, 409600.0]\n",
      "[764341.0847167969, 409600.0]\n",
      "[760809.306640625, 409600.0]\n",
      "[756883.6325683594, 409600.0]\n",
      "[753190.5478515625, 409600.0]\n",
      "[749643.9189453125, 409600.0]\n",
      "[746230.5986328125, 409600.0]\n",
      "[742712.3239746094, 409600.0]\n",
      "epoch: 40, log ppl: 1.8132625097036361, predict: time traveller    and the was the courd  and\n",
      "[739411.2507324219, 409600.0]\n",
      "[736027.5673828125, 409600.0]\n",
      "[732722.4758300781, 409600.0]\n",
      "[729466.5583496094, 409600.0]\n",
      "[726574.8767089844, 409600.0]\n",
      "[723386.7568359375, 409600.0]\n",
      "[720385.7612304688, 409600.0]\n",
      "[717573.6906738281, 409600.0]\n",
      "[714734.2216796875, 409600.0]\n",
      "[711919.0280761719, 409600.0]\n",
      "epoch: 50, log ppl: 1.7380835646390915, predict: time traveller  and the courd  i had the roo\n",
      "log perplexity 1.7\n",
      "time traveller  and the courd  i had the roo\n",
      "traveller  and the courd  i had the roo\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 50, 0.1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.GRU\n",
    "\n",
    "when calculate gradient in RNNs, long products of matrices can lead to vanishing or exploding gradients.\n",
    "\n",
    "A number of methods have been proposed to address this, one is gru.\n",
    "\n",
    "The key distinction between RNNs and GRUs is that the latter support gating of the hidden state\n",
    "\n",
    "This means that we have mechanisms for when a hidden state should be updated and also when it should be reset\n",
    "\n",
    "For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 reset gate and update gate:\n",
    "\n",
    "![jupyter](./gru-1.svg)\n",
    "\n",
    "equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_t = \\sigma(X_t W_{xr} + H_{t-1} W_{hr} + b_r),\\\\\n",
    "Z_t = \\sigma(X_t W_{xz} + H_{t-1} W_{hz} + b_z),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $X_{t} \\in \\mathbb{R}^{n \\times d}, W_{xr},W_{xz} \\in \\mathbb{R}^{d \\times h}, H_{t-1} \\in \\mathbb{R}^{n \\times h}, W_{hr},W_{hz} \\in \\mathbb{R}^{h \\times h}, b_r,b_z \\in \\mathbb{R}^{1 \\times h}$.\n",
    "\n",
    "finally $R_t, Z_t \\in \\mathbb{R}^{n \\times h}$ with elements in $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 candidate hidden state\n",
    "\n",
    "integrate the reset gate  $R_{t}$  with the regular latent state updating:\n",
    "\n",
    "$$\\tilde{H}_{t} = tanh(X_{t}W_{xh} + (R_{t} \\odot H_{t-1})W_{hh} + b_{h})$$\n",
    "\n",
    "the difference between RNNs and GRUs:\n",
    "\n",
    "$$H_{t-1} \\to R_{t} \\odot H_{t-1}$$\n",
    "\n",
    "reset gate: control how much of the previous state we might still want to remember, better capture short dependencies.\n",
    "\n",
    "RNNs: how much to remember is fixed along all steps. \n",
    "\n",
    "GRUs: how much to remember is flexible determined by $(X_{t}, H_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 hidden state\n",
    "\n",
    "incorporate the effect of the update gate  $Z_{t}$. \n",
    "\n",
    "$$H_{t} = Z_{t} \\odot H_{t-1} + (1 - Z_{t}) \\odot \\tilde{H}_{t}$$\n",
    "\n",
    "$Z_{t}$  is close to 1: we simply retain the old state, In this case the information from  $X_{t}$  is essentially ignored.\n",
    "\n",
    "$Z_{t}$ is close to 0: $H_{t}$  approaches the candidate latent state  $\\tilde{H}_{t}$\n",
    "\n",
    "this design: better capture dependencies for sequences with large time step distances.\n",
    "\n",
    "![jupyter](./gru-3.svg)\n",
    "\n",
    "reset gate $R_{t}$: inner, short dependencies.\n",
    "\n",
    "update gate $Z_{t}$: outer, long dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 gru implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = nn.GRU(len(vocab), num_hiddens)\n",
    "gru_net = RNNModel(gru_layer, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1055053.6240234375, 409600.0]\n",
      "[944187.3481445312, 409600.0]\n",
      "[895549.3020019531, 409600.0]\n",
      "[859744.2561035156, 409600.0]\n",
      "[832119.8303222656, 409600.0]\n",
      "[809264.7875976562, 409600.0]\n",
      "[789003.4211425781, 409600.0]\n",
      "[770029.1254882812, 409600.0]\n",
      "[752922.2001953125, 409600.0]\n",
      "[735960.9733886719, 409600.0]\n",
      "epoch: 10, log ppl: 1.7967797201871871, predict: time traveller  and the sood  and the sood  \n",
      "[720925.8706054688, 409600.0]\n",
      "[706715.208984375, 409600.0]\n",
      "[693712.6833496094, 409600.0]\n",
      "[681708.9838867188, 409600.0]\n",
      "[670652.5776367188, 409600.0]\n",
      "[660365.0559082031, 409600.0]\n",
      "[650396.6463623047, 409600.0]\n",
      "[641452.8891601562, 409600.0]\n",
      "[632858.6715087891, 409600.0]\n",
      "[624666.8208007812, 409600.0]\n",
      "epoch: 20, log ppl: 1.5250654804706574, predict: time traveller  and the starder  and the sta\n",
      "[617160.4501953125, 409600.0]\n",
      "[609748.3176269531, 409600.0]\n",
      "[602846.6145019531, 409600.0]\n",
      "[596218.3585205078, 409600.0]\n",
      "[589681.0755615234, 409600.0]\n",
      "[583643.8028564453, 409600.0]\n",
      "[577619.9357910156, 409600.0]\n",
      "[572026.0272216797, 409600.0]\n",
      "[566473.2233886719, 409600.0]\n",
      "[561250.3443603516, 409600.0]\n",
      "epoch: 30, log ppl: 1.3702400985360146, predict: time travellers  and the white rabbit   said\n",
      "[555925.8385009766, 409600.0]\n",
      "[551356.9229736328, 409600.0]\n",
      "[546300.5798339844, 409600.0]\n",
      "[541091.8099365234, 409600.0]\n",
      "[536569.4438476562, 409600.0]\n",
      "[531776.2963867188, 409600.0]\n",
      "[526994.3187255859, 409600.0]\n",
      "[522590.6600341797, 409600.0]\n",
      "[518675.98083496094, 409600.0]\n",
      "[513428.6591796875, 409600.0]\n",
      "epoch: 40, log ppl: 1.2534879374504089, predict: time traveller with her head   said alice   \n",
      "[509334.9093017578, 409600.0]\n",
      "[505136.2897949219, 409600.0]\n",
      "[500829.0048828125, 409600.0]\n",
      "[496479.05139160156, 409600.0]\n",
      "[492524.36962890625, 409600.0]\n",
      "[489080.8903808594, 409600.0]\n",
      "[484491.92236328125, 409600.0]\n",
      "[482677.0861816406, 409600.0]\n",
      "[478367.7576904297, 409600.0]\n",
      "[474848.05212402344, 409600.0]\n",
      "epoch: 50, log ppl: 1.1592970022559166, predict: time traveller with her  here   i deed it so\n",
      "log perplexity 1.2\n",
      "time traveller with her  here   i deed it so\n",
      "travellers  and the morning  she should\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 50, 1\n",
    "train_ch8(gru_net, train_iter, vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LSTM\n",
    "\n",
    "challenges: long-term information preservation and short-term input skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 input gate, forget gate, output gate\n",
    "\n",
    "equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I_t = \\sigma(X_t W_{xi} + H_{t-1} W_{hi} + b_i),\\\\\n",
    "F_t = \\sigma(X_t W_{xf} + H_{t-1} W_{hf} + b_f),\\\\\n",
    "O_t = \\sigma(X_t W_{xo} + H_{t-1} W_{ho} + b_o)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $X_{t} \\in \\mathbb{R}^{n \\times d}, W_{xi},W_{xf},W_{xo} \\in \\mathbb{R}^{d \\times h}, H_{t-1} \\in \\mathbb{R}^{n \\times h}, W_{hi},W_{hf},W_{ho} \\in \\mathbb{R}^{h \\times h}, b_i,b_f,b_o \\in \\mathbb{R}^{1 \\times h}$.\n",
    "\n",
    "finally $I_t, F_t, O_t \\in \\mathbb{R}^{n \\times h}$ with elements in $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 memory cell\n",
    "\n",
    "we first have the candidate memory cell using the tanh activation:\n",
    "\n",
    "$$\\tilde{C}_{t} = tanh(X_t W_{xc} + H_{t-1} W_{hc} + b_c)$$\n",
    "\n",
    "![jupyter](./lstm-1.svg)\n",
    "\n",
    "forget gate $F_{t}$: addresses how much of the old memory cell content  $C_{t-1}$  we retain.\n",
    "\n",
    "input gate $I_{t}$: how much we take new data into account via  $\\tilde{C}_{t}$\n",
    "\n",
    "$$C_{t} = F_{t} \\odot C_{t-1} + I_{t} \\odot \\tilde{C}_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 hidden state\n",
    "\n",
    "output gate: how to use memory in hidden state.\n",
    "\n",
    "$$H_{t} = O_{t} \\odot tanh(C_{t})$$\n",
    "\n",
    "$H_{t}$ is therefore in the interval $(-1, 1)$.\n",
    "\n",
    "![jupyter](./lstm-3.svg)\n",
    "\n",
    "output gate $O_{t}$: outer, long term dependencies.\n",
    "\n",
    "input gate $I_{t}$, forget gate $F_{t}$: inner, short term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 lstm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
    "lstm_net = RNNModel(lstm_layer, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1118939.3466796875, 409600.0]\n",
      "[988705.4716796875, 409600.0]\n",
      "[930641.1115722656, 409600.0]\n",
      "[883755.7602539062, 409600.0]\n",
      "[848321.111328125, 409600.0]\n",
      "[822815.9350585938, 409600.0]\n",
      "[801239.1000976562, 409600.0]\n",
      "[782676.5886230469, 409600.0]\n",
      "[766345.4780273438, 409600.0]\n",
      "[750910.0705566406, 409600.0]\n",
      "epoch: 10, log ppl: 1.8332765394449233, predict: time traveller  and the could  i was the cou\n",
      "[737386.3405761719, 409600.0]\n",
      "[724549.5661621094, 409600.0]\n",
      "[712904.8166503906, 409600.0]\n",
      "[701851.7102050781, 409600.0]\n",
      "[691463.306640625, 409600.0]\n",
      "[682094.59765625, 409600.0]\n",
      "[672885.2631835938, 409600.0]\n",
      "[664293.7593994141, 409600.0]\n",
      "[656162.9904785156, 409600.0]\n",
      "[648605.845703125, 409600.0]\n",
      "epoch: 20, log ppl: 1.583510365486145, predict: time traveller  and the compress  and the co\n",
      "[641640.5600585938, 409600.0]\n",
      "[634418.2532958984, 409600.0]\n",
      "[628169.5251464844, 409600.0]\n",
      "[621698.1871337891, 409600.0]\n",
      "[615903.4088134766, 409600.0]\n",
      "[609829.5883789062, 409600.0]\n",
      "[604449.1677246094, 409600.0]\n",
      "[599121.3454589844, 409600.0]\n",
      "[593783.3446044922, 409600.0]\n",
      "[588595.7874755859, 409600.0]\n",
      "epoch: 30, log ppl: 1.4370014342665671, predict: time traveller  and the looked and short of \n",
      "[583753.9373779297, 409600.0]\n",
      "[579303.849609375, 409600.0]\n",
      "[574954.8228759766, 409600.0]\n",
      "[569557.9891357422, 409600.0]\n",
      "[565623.1851806641, 409600.0]\n",
      "[561137.6087646484, 409600.0]\n",
      "[557291.4429931641, 409600.0]\n",
      "[552843.8931884766, 409600.0]\n",
      "[548530.5540771484, 409600.0]\n",
      "[545071.6010742188, 409600.0]\n",
      "epoch: 40, log ppl: 1.3307412135601044, predict: time traveller  i was not to be sure    she \n",
      "[540940.2564697266, 409600.0]\n",
      "[536765.9791259766, 409600.0]\n",
      "[534132.7625732422, 409600.0]\n",
      "[529550.6361083984, 409600.0]\n",
      "[526436.7130126953, 409600.0]\n",
      "[523581.6643066406, 409600.0]\n",
      "[520430.9865722656, 409600.0]\n",
      "[517273.54846191406, 409600.0]\n",
      "[513214.5441894531, 409600.0]\n",
      "[510472.5018310547, 409600.0]\n",
      "epoch: 50, log ppl: 1.2462707564234734, predict: time traveller  i should have starking the s\n",
      "log perplexity 1.2\n",
      "time traveller  i should have starking the s\n",
      "traveller  i should have starking the s\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 50, 1\n",
    "train_ch8(lstm_net, train_iter, vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
